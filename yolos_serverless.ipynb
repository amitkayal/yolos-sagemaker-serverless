{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YOLOS Object Deployment with SageMaker Serverless Inference\n",
    "\n",
    "In this notebook I will show you how to deploy a YOLOS Object Detection model with Hugging Face and AWS SageMaker.\n",
    "The process mainly follows that of [Phillip Schmid's notebook](https://github.com/huggingface/notebooks/tree/main/sagemaker/19_serverless_inference)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "\n",
    "from sagemaker.pytorch.model import PyTorchModel\n",
    "from sagemaker.huggingface.model import HuggingFaceModel, HuggingFacePredictor\n",
    "from sagemaker.serverless import ServerlessInferenceConfig\n",
    "from sagemaker.serializers import DataSerializer\n",
    "\n",
    "import io\n",
    "from PIL import Image\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AWS Setup\n",
    "After all the imports, we need to set up some AWS specific variables. I usually run this code locally, if you are using SageMaker Notebooks you will not need to specifiy all of the below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boto3_sess = boto3.Session(profile_name=os.environ.get[\"SAGEMAKER_PROFILE\"])\n",
    "sess = sagemaker.Session(boto_session=boto3_sess)\n",
    "\n",
    "role = os.environ.get[\"SAGEMAKER_ROLE\"]\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the YOLOS weights into S3\n",
    "Next is getting the YOLOS weights from the Hugging Face Hub and ultimately uploading them to an S3 Bucket.\n",
    "\n",
    "First we define the Hub repository path where the weights are stored and the S3 location where we want to upload them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repository = \"hustvl/yolos-tiny\"\n",
    "model_id=repository.split(\"/\")[-1]\n",
    "s3_location=f\"s3://{sess.default_bucket()}/custom_inference/{model_id}/model.tar.gz\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we use git (lfs) to clone the weight repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git lfs install\n",
    "!git clone https://huggingface.co/$repository"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next line is special for the transformer library version < 4.20.1. As YOLOS was only added there we need to copy the custom inference script, located in *code* to the cloned model repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp -r code/ $model_id/code/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we pack the model weights and all the other files in the repo into a *tar.gz* and finally upload it via AWS CLI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd $model_id\n",
    "!tar zcvf model.tar.gz *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 --profile celapp cp model.tar.gz $s3_location\n",
    "%cd .. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define and Deploy to SageMaker Serverless Inference\n",
    "In the next step we define everything necessary for the serverless endpoint, starting with the *HuggingFaceModel*. \n",
    "Next we define some properties, i.e. the max memory size and number of maximum concurrent calls/processes of the endpoint.\n",
    "To make it easy to handle image data we also create a *DataSerializer*, which is then passed to the *deploy* method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "huggingface_model = HuggingFaceModel(\n",
    "    model_data=s3_location, \n",
    "    sagemaker_session=sess,\n",
    "    role=role,                   \n",
    "    transformers_version=\"4.17\", \n",
    "    pytorch_version=\"1.10\",        # pytorch version used\n",
    "    py_version='py38',            # python version used\n",
    ")\n",
    "\n",
    "# Specify MemorySizeInMB and MaxConcurrency in the serverless config object\n",
    "serverless_config = ServerlessInferenceConfig(\n",
    "    memory_size_in_mb=4096, max_concurrency=10,\n",
    ")\n",
    "\n",
    "image_serializer = DataSerializer(content_type='image/x-image') \n",
    "\n",
    "# deploy the endpoint endpoint\n",
    "yolos_predictor = huggingface_model.deploy(\n",
    "    endpoint_name=\"yolos-t-object-detection-serverless\",\n",
    "    serverless_inference_config=serverless_config,\n",
    "    serializer=image_serializer\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Call the endpoint\n",
    "We can simply call the the endpoint by invoking *predict* on the predictor we got out of the *deploy* method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = yolos_predictor.predict(data=\"example_resized.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean up\n",
    "If you're just testing and don't want the model and endpoint to hang around in your SageMaker account, run the following lines to clean them up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yolos_predictor.delete_model()\n",
    "yolos_predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result Visualization\n",
    "Use the below code to visualize your object detections.\n",
    "You can also have a look in the *infer_image.py* file, which I used for a more lightweight inference, that does not need to load the YOLOS model from the transformers library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from transformers import YolosForObjectDetection\n",
    "\n",
    "# colors for visualization\n",
    "COLORS = [[0.000, 0.447, 0.741], [0.850, 0.325, 0.098], [0.929, 0.694, 0.125],\n",
    "          [0.494, 0.184, 0.556], [0.466, 0.674, 0.188], [0.301, 0.745, 0.933]]\n",
    "model = YolosForObjectDetection.from_pretrained(\"hustvl/yolos-small\")\n",
    "\n",
    "def plot_results(pil_img, prob, boxes):\n",
    "    plt.figure(figsize=(16,10))\n",
    "    plt.imshow(pil_img)\n",
    "    ax = plt.gca()\n",
    "    colors = COLORS * 100\n",
    "    for p, (xmin, ymin, xmax, ymax), c in zip(prob, boxes, colors):\n",
    "        ax.add_patch(plt.Rectangle((xmin, ymin), xmax - xmin, ymax - ymin,\n",
    "                                   fill=False, color=c, linewidth=3))\n",
    "        cl = p.argmax()\n",
    "        text = f'{model.config.id2label[cl.item()]}: {p[cl]:0.2f}'\n",
    "        ax.text(xmin, ymin, text, fontsize=15,\n",
    "                bbox=dict(facecolor='yellow', alpha=0.5))\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = Image.open(\"example_resized.jpg\")\n",
    "plot_results(image, np.asarray(res[\"probabilities\"]), res[\"bounding_boxes\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.9 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "61699ddb3f951ddfd1a89f8af5fc7f9b06ca1851dd616fb3fa77053aefa67f93"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
